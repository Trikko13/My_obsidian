**DAG — это ориентированный ациклический граф**, концептуальное представление серии действий или, другими словами, математическая абстракция конвейера данных (data pipeline).

Хотя оба термина, DAG и data pipeline, используются в разных сферах, они представляют собой почти идентичный механизм. В двух словах, DAG (или конвейер) определяет последовательность этапов выполнения в любом неповторяющемся алгоритме.

**1)**                **Какие операторы использовал?**

**Операторы** являются основными строительными блоками DAG Airflow. Это классы, которые содержат логику выполнения единичной работы.

Вы можете использовать операторы в Airflow, создав их экземпляры в задачах. Задача определяет работу, выполняемую оператором в контексте DAG.

Хуки (Hooks) являются одним из основных строительных блоков Airflow. На высоком уровне хук - это абстракция определенного API, который позволяет Airflow взаимодействовать с внешней системой. Хуки встроены во многие операторы, но их также можно использовать непосредственно в коде DAG.

**BashOperator** используется для запуска простых bash комманд.

**PythonOperator** вызывает функцию python, определенную ранее в нашем коде.

**PostgresOperator** выдает инструкцию SQL для базы данных Postgres. Учетные данные для базы данных хранятся в соединении Airflow с именем my_postgres_connection. Если вы посмотрите на код [оператора Postgres](https://registry.astronomer.io/providers/postgres/modules/postgresoperator), он использует [PostgresHook](https://registry.astronomer.io/providers/postgres/modules/postgreshook) для взаимодействия с базой данных.

## 2)                Приходилось ли использовать сенсоры?

**Сенсоры** - это особый вид операторов. Когда они выполняются, они проверяют, выполняется ли определенный критерий, прежде чем разрешить выполнение последующих задач. Это отличный способ заставить ожидать часть задач вашего DAG завершения какого-либо внешнего процесса или выполнения условия.

·                     **DateTimeSensor** ожидает прохождения указанной даты и времени. Полезно, когда нужно выполнять задачи из одного DAG в разное время;

·                     **HttpSensor** ожидает доступности API;

·                     **SqlSensor** ожидает появления данных в таблице SQL. Пригодится, если нужно, чтобы DAG обрабатывал данные по мере их поступления в базу данных.

**3)**    **Как тестировал даги?**

Для тестирования дагов в Apache Airflow можно использовать модуль **unittest** в сочетании с классом **DagBag**. Вот пример того, как вы можете написать тест для вашего дага:

**Создайте новый файл** с расширением .py для вашего теста дага.

**Импортируйте** необходимые модули:

import **unittest**

from airflow.models import **DagBag**

**Создайте** **класс** **теста**, который наследуется от **unittest.TestCase**:

class TestMyDag(unittest.TestCase):

    def setUp(self):

        self.dagbag = DagBag()

    def test_dag_loaded(self):

        dag_id = 'my_dag_id'

        dag = self.dagbag.get_dag(dag_id)

        self.assertIsNotNone(dag)

В методе **setUp** инициализируйте экземпляр **DagBag**, который будет загружать все даги из вашего проекта.

В методе **test_dag_loaded** проверьте, что ваш даг был успешно загружен из **DagBag**.

**Запустите** ваш тест с помощью команды **python -m unittest <имя_файла_теста>**. Например, **python -m unittest test_my_dag.py**.

При запуске теста, он загрузит все даги из вашего проекта и проверит, что ваш даг был успешно загружен. Вы также можете добавить дополнительные тесты для проверки других аспектов вашего дага, таких как наличие операторов, правильность зависимостей и т.д.


### Вопрос: "Где разворачивали Airflow, откуда и куда гнали данные?"
👉 **Ты сказал, что из HDFS и API → в DWH на Hadoop**.

🔹 **Корректно ли?**

- **HDFS** — ОК, но **обычно данные из HDFS не считаются DWH**. Hadoop — скорее **Data Lake**.
- **API → DWH** — тоже возможно, но обычно API-данные идут в **стейджинг (staging)** перед DWH.
- **Обычно DWH разворачивается в Greenplum, Snowflake, Redshift**, но иногда Hadoop + Hive могут использоваться для OLAP.

🔹 **Как лучше было ответить?**  
👉 **Ответ:**

> Airflow был развернут на **сервере Linux в кластере Hadoop**.  
> Данные брали **из HDFS, API, Kafka**, загружали в **DWH (Greenplum / Snowflake / Hive)**.

📌 **Корректный стек DWH:**

- Источники: **HDFS, API, Kafka, Clickhouse, PostgreSQL**
- Промежуточное хранение: **Staging в Data Lake (HDFS / S3)**
- DWH: **Greenplum / Snowflake / Redshift**

💡 **Совет:** Если не уверен, уточни:

> "Зависит от архитектуры компании: где-то используют Hadoop + Hive как DWH, а где-то Greenplum / Snowflake."

### Вопрос: "Какие библиотеки нужны для работы с Hadoop?"
👉 **Ты сказал, что пишешь `hdfs.read.parquet('sentence')`, но тебе сказали, что это неверно.**

🔹 **Какие библиотеки действительно используются?**  
✅ **PyArrow** (`pyarrow.fs.HadoopFileSystem`)  
✅ **hdfs3 / fsspec** (работа с HDFS)  
✅ **Pyspark** (`spark.read.parquet()`)

🔹 **Как правильно читать Parquet из HDFS?**


`from pyspark.sql import SparkSession  
`spark = SparkSession.builder.appName("ReadHDFS").getOrCreate()

`df = spark.read.parquet("hdfs://namenode:9000/data/file.parquet") df.show()`

📌 **Вывод:**

- Для HDFS в Spark → `spark.read.parquet("hdfs://...")`
- Для Pandas → лучше **использовать `pyarrow` или `fsspec`**
- `hdfs.read.parquet('sentence')` **неверно**, т.к. Pandas не умеет работать напрямую с HDFS
### Вопрос: "Когда понижать количество партиций?"
👉 **Ты сказал "при записи файла", что частично верно, но не объяснил критерии.**

✅ **Понижать партиции (`coalesce()`) нужно, если:**  
1️⃣ **Файлы слишком маленькие** → т.к. много мелких файлов **замедляют работу HDFS и Spark**.  
2️⃣ **Слишком много файлов на запись** → это снижает производительность.

🔹 **Пример: Уменьшаем партиции перед записью (чтобы было меньше файлов):**

`df = df.coalesce(1)  # Объединяем партиции df.write.parquet("hdfs://namenode:9000/output/")`

📌 **Почему?**

- Если каждая партиция пишет отдельный файл, то 1000 партиций = 1000 файлов → медленно!
- `coalesce(1)` → **создаёт 1 файл, но медленнее выполняется**.
#### Вопрос: "Что такое SCD?"
**Ты назвал `SCD0`, `SCD1`, `SCD2`, но запутался в таблицах.**

✅ **Slowly Changing Dimensions (SCD) → метод хранения исторических данных в DWH.**

|**Тип**|**Описание**|**Как реализуется?**|
|---|---|---|
|**SCD 0**|Нет историчности, данные просто обновляются|`UPDATE`|
|**SCD 1**|Перезаписываем данные без хранения истории|`UPDATE` (без версий)|
|**SCD 2**|Добавляем новую строку для каждой версии записи|`valid_from`, `valid_to`|
|**SCD 3**|Добавляем колонку с "предыдущим значением"|`prev_value`|

🔹 **Пример SCD 2 (историзация в одной таблице):**

sql

Копировать

`CREATE TABLE customers (     id INT PRIMARY KEY,     name VARCHAR,     address VARCHAR,     valid_from DATE,     valid_to DATE NULL );`

📌 **Вывод:**

- **SCD ≠ отдельные таблицы** → историчность ведётся **в тех же таблицах, но с доп. колонками**.


### Вопрос: "Как работает `broadcast join`?"

**Ты сказал, что "малая таблица копируется на кластера", что частично верно, но лучше точнее.**

✅ **Как это работает?**

- **Spark копирует маленькую таблицу (`< 100MB`) на все узлы**.
- У каждого executor'а есть копия этой таблицы → **ускоряет join**.

🔹 **Пример `broadcast join` в Spark:**



`from pyspark.sql.functions import broadcast  

`big_df = spark.read.parquet("hdfs://big_table.parquet") 
`small_df = spark.read.csv("hdfs://small_table.csv")  

`df_joined = big_df.join(broadcast(small_df), "id")  # Оптимизированный join`

📌 **Почему это ускоряет работу?**

- Без `broadcast()` Spark делает **shuffle join** (медленный, т.к. нужно перемещать данные).
- `broadcast()` убирает shuffle и **ускоряет join в разы**.