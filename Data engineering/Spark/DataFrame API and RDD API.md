### **1. RDD API (Resilient Distributed Dataset)**

#### **Особенности:**

1. **Низкоуровневое API:**
    
    - Предоставляет доступ к распределённым данным на самом базовом уровне.
    - Обеспечивает полный контроль над распределением данных и процессом вычислений.
2. **Не структурировано:**
    
    - Работает с обычными объектами Python (или Java, Scala).
    - Не поддерживает схему данных (schema), как в DataFrame API.
3. **Ленивые вычисления:**
    
    - Операции выполняются только при вызове действия (action), как и в DataFrame API.

#### **Когда использовать RDD:**

- Если нужно полностью управлять процессом вычислений.
- Для работы с неструктурированными данными или нестандартными источниками.

#### **Пример работы с RDD:**

`from pyspark import SparkContext  

`sc = SparkContext("local", "RDD Example")  

`# Создание RDD из списка 
`data = [1, 2, 3, 4, 5] 
`rdd = sc.parallelize(data)  

`# Применение трансформации и действия 
`squared_rdd = rdd.map(lambda x: x ** 2) print(squared_rdd.collect())  # [1, 4, 9, 16, 25]`

---

### **2. DataFrame API**

#### **Особенности:**

1. **Высокоуровневое API:**
    
    - Основан на понятиях таблиц и схем, как в базах данных.
    - Поддерживает колонки, типы данных, и операции, похожие на SQL.
2. **Оптимизация через Catalyst:**
    
    - DataFrame API тесно интегрирован с Catalyst Optimizer, который автоматически оптимизирует запросы.
3. **Структурированность:**
    
    - Работает с табличными данными.
    - Подходит для аналитики и интеграции с SQL.
4. **Богатый функционал:**
    
    - Поддерживает агрегаты, оконные функции, работу с несколькими источниками данных (CSV, Parquet, Hive).

#### **Когда использовать DataFrame API:**

- Для работы с большими структурированными данными.
- Когда важна производительность и удобство написания кода.

#### **Пример работы с DataFrame API:**

python

Копировать код

`from pyspark.sql import SparkSession  
`spark = SparkSession.builder.master("local").appName("DataFrame Example").getOrCreate()  

`# Создание DataFrame из списка 
`data = [("Alice", 34), ("Bob", 45), ("Cathy", 29)] 
`df = spark.createDataFrame(data, ["Name", "Age"])  

`# Применение фильтрации и выборки 
`filtered_df = df.filter(df["Age"] > 30).select("Name") filtered_df.show()  

`# Результат: 
`# +-----+ 
`# | Name| 
`# +-----+ 
`# |Alice| 
`# |  Bob| 
`# +-----+`

---

### **Ключевые различия между RDD API и DataFrame API**

|**Функция**|**RDD API**|**DataFrame API**|
|---|---|---|
|**Абстракция**|Низкоуровневая (работа с объектами).|Высокоуровневая (работа с таблицами).|
|**Тип данных**|Нет схемы, данные неструктурированы.|Имеет схему, типы данных как в таблицах.|
|**Производительность**|Меньше оптимизаций, требует больше ресурсов.|Использует Catalyst для автоматической оптимизации.|
|**Удобство**|Гибкость, но сложность написания и поддержки кода.|Более интуитивный синтаксис, подходящий для аналитики.|
|**Интеграция с SQL**|Не поддерживает.|Полностью поддерживает SQL-запросы.|
|**Когда использовать**|Нестандартные данные или сложная логика обработки.|Структурированные данные, аналитические задачи.|

---

### **Как они связаны?**

- DataFrame API построен поверх RDD API. Это значит, что DataFrame в конечном итоге преобразуется в RDD для выполнения.
- DataFrame API добавляет схему, Catalyst Optimizer и множество удобных функций поверх базового RDD.

---

### **Итог**

- **RDD API:** Используй, если нужна полная гибкость или работаешь с неструктурированными данными.
- **DataFrame API:** Предпочитай для аналитики и высокоуровневой работы с табличными данными, особенно если требуется производительность и удобство.