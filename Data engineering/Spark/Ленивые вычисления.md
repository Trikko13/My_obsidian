### **Ленивые вычисления (Lazy Evaluation) в Spark**

**Ленивые вычисления** — это концепция в Spark, при которой операции **трансформации** (например, `map`, `filter`) не выполняются сразу. Вместо этого Spark откладывает их выполнение до тех пор, пока не будет вызвана операция **действия** (например, `collect`, `count`).

---

### **Как это работает?**

1. **Трансформации:**
    
    - Каждая трансформация на RDD или DataFrame добавляет шаг в граф операций, называемый **DAG (Directed Acyclic Graph)**.
    - Эти шаги записываются, но не выполняются сразу.
2. **Действия:**
    
    - Когда вызывается действие, Spark начинает выполнять все накопленные трансформации.
    - DAG преобразуется в физические задачи (**jobs**) и распределяется между узлами кластера для выполнения.

---

### **Почему используется ленивое выполнение?**

1. **Оптимизация:**
    
    - Spark анализирует все накопленные трансформации и оптимизирует их перед выполнением.
    - Например, **слияние нескольких фильтров в один** или **перестановка операций для минимизации shuffle**.
2. **Снижение накладных расходов:**
    
    - Нет необходимости выполнять каждую операцию сразу. Это экономит ресурсы, особенно если результат трансформации не используется.
3. **Гибкость:**
    
    - Spark может строить оптимальный план выполнения, основываясь на всей последовательности операций.

---

### **Типы операций**

1. **Трансформации (Transformations):**
    
    - Изменяют существующий RDD или DataFrame и возвращают новый RDD/DataFrame.
    - **Ленивые:** они выполняются только при вызове действия.
    - Примеры: `map`, `filter`, `join`, `groupByKey`.
2. **Действия (Actions):**
    
    - Запускают выполнение трансформаций, накопленных в DAG.
    - Возвращают результат в драйвер или сохраняют его на диск.
    - Примеры: `collect`, `count`, `saveAsTextFile`, `show`.

---

### **Пример кода**

```python
from pyspark.sql import SparkSession

# Создаём SparkSession
spark = SparkSession.builder.appName("Lazy Evaluation").getOrCreate()

# Загружаем данные
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
columns = ["name", "age"]
df = spark.createDataFrame(data, schema=columns)

# Трансформации (ленивые)
filtered_df = df.filter(df["age"] > 30)  # Фильтрация
selected_df = filtered_df.select("name")  # Выбор столбцов

# До вызова действия Spark ничего не выполняет
print("Трансформации записаны, но не выполнены")

# Действие (запускает выполнение)
result = selected_df.collect()  # Сбор результатов в драйвер
print(result)
```

---

### **Этапы выполнения**

1. **Создание DAG:**
    
    - При вызове `filter` и `select`, Spark добавляет шаги в DAG.
    - Данные ещё не фильтруются и не выбираются.
2. **Запуск DAG:**
    
    - При вызове `collect`, Spark начинает выполнение DAG:
        - Фильтрует строки, где `age > 30`.
        - Выбирает столбец `name`.
        - Отправляет результат в драйвер.

---

### **Оптимизации ленивого выполнения**

1. **Pipelining:**
    
    - Spark объединяет несколько трансформаций в один этап, чтобы избежать промежуточного shuffle.
    
    ```python
    df.filter(df["age"] > 30).select("name")
    ```
    
    Это выполняется как одна операция, а не две.
    
2. **Predicate Pushdown:**
    
    - Если данные читаются из источника (например, Parquet), фильтрация (`filter`) выполняется ещё на этапе чтения, уменьшая объём данных, которые Spark загружает в память.
3. **Column Pruning:**
    
    - Spark выбирает только нужные столбцы на этапе чтения, чтобы не загружать ненужные данные.

---

### **Преимущества ленивых вычислений**

1. **Повышение производительности:**
    
    - Spark выполняет минимальное количество операций для достижения результата.
    - Пример: Если результат трансформации не используется, она не выполняется.
2. **Уменьшение затрат на shuffle:**
    
    - Spark оптимизирует DAG так, чтобы минимизировать передачи данных между узлами.
3. **Экономия ресурсов:**
    
    - Операции выполняются только тогда, когда это необходимо, что снижает нагрузку на кластер.

---

### **Итог**

Ленивые вычисления — это ключевая особенность Spark, которая делает его мощным инструментом. Они позволяют:

- Отложить выполнение трансформаций до вызова действий.
- Оптимизировать выполнение задач.
- Эффективно использовать ресурсы кластера.