### **Задача 1: Работа с числами**

**Цель:** Создать RDD, выполнить несколько трансформаций и действий.

#### **Шаги:**

1. Создайте RDD из списка чисел `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`.
2. Уберите нечётные числа.
3. Умножьте оставшиеся числа на 10.
4. Соберите результат в список и выведите его.

Решение:

`from pyspark.sql import SparkSession`

`#создадим SparkSession локально с именем и присвоим значение переменной spark`

`spark=SparkSession.builder.master('local').appName('My_first_RDD').getOrCreate()`

`#обращаемся SparkContext т.к. он уже внутри SparkSession и без него RDD не сделать`

`sc = spark.sparkContext`

`Список для создания RDD`

`dataset = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`
`rdd = sc.parallelize(dataset)`

`Применим трансорфмацию`
`even_numbers_rdd = rdd.filter(lambda x: x % 2 == 0)`

`Собираем результат`
`print(even_numbers_rdd.collect())`

### **Почему использовать `filter`, а не `map`?**

- **`map`**: Преобразует каждый элемент RDD, возвращая для него одно значение или структуру. Если вы используете `map` с `dataset`, это создаёт списки для каждого элемента.
- **`filter`**: Отбирает только те элементы RDD, которые соответствуют условию (в данном случае, чётные числа).

### **Сравнение `map` и `filter`:**

|**Функция**|**Что делает**|**Когда использовать**|
|---|---|---|
|**`map`**|Преобразует каждый элемент RDD|Когда нужно изменить все элементы|
|**`filter`**|Убирает элементы, которые не соответствуют условию|Когда нужно оставить только определённые элементы|
### **Задача 2: Работа с текстом**

**Цель:** Обработать строки текста, найти слова и подсчитать их длину.

#### **Шаги:**

1. Создайте RDD из списка строк:
`["Spark is great", "I love big data", "RDD is powerful"]
2. Разбейте строки на отдельные слова.
3. Вычислите длину каждого слова
4. Соберите и выведите результат.
   
`from pyspark.sql import SparkSession`

`#Создаём SparkSession`
`spark=SparkSession.builder.master('local').appName('Task_2').getOrCreate()`

`#Получаем SparkContext`
`sc = spark.sparkContext`

`#Создаём RDD из строк`
`data = ["Spark is great", "I love big data", "RDD is powerful"]`
`rdd = sc.parallelize(data)`

`#Разбиваем строки на слова`
`words_rdd = rdd.flatMap(lambda line: line.split())`

`#Вычисялемд лину кжадого слова`
`word_length_rdd = words_rdd.map(lambda word: (word,len(word)))`

`print(word_length_rdd.collect())`

`#Останавливаем SparkSession`
`spark.stop()`

### **Задача 3: Подсчёт уникальных элементов**

**Цель:** Определить количество уникальных чисел в списке.

#### **Шаги:**

1. Создайте RDD из списка чисел с дубликатами:
    [1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 7]
- Удалите дубликаты.
- Подсчитайте количество уникальных чисел.


`from pyspark.sql import SparkSession`

`#Создаём SparkSession`
`spark=SparkSession.builder.master('local').appName('Task_3').getOrCreate()`

`#Получаем SparkContext`
`sc = spark.sparkContext`

`#Создаём RDD из строк`
`data = [1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 7]`
`rdd = sc.parallelize(data)`

`#Удаляем дубликаты`
`unique_rdd = rdd.distinct()`

`#Подсчитываем количество уникальных чисел`
`unique_count =  unique_rdd.count()`
`print(f'Количество уникальных чисел: {unique_count}')` 

`#Останавливаем SparkSession`
`spark.stop()`


### **Задача 4: Подсчёт частоты слов**

**Цель:** Найти, сколько раз каждое слово встречается в списке строк..

#### **Шаги:**

1. Создайте RDD из cток:
    - `["hello world", "hello spark", "hello big data"]`
- Разбейте строки на слова.
- Подсчитайте, сколько раз встречается каждое слово.
- 