### **Что учить для собеседований:**

1. **Работа с DataFrame API:**
    
    - Основные трансформации: `filter`, `select`, `withColumn`, `groupBy`, `agg`.
    - Соединения: `join`.
    - Агрегации: `count`, `avg`, `sum`.
2. **Оптимизация в Spark:**
    
    - Кэширование: `.cache()` и `.persist()`.
    - Партиционирование: `repartition`, `coalesce`.
    - Понимание шифлов (shuffle).
3. **SQL в Spark:**
    
    - Регистрация таблиц: `createOrReplaceTempView`.
    - Выполнение простых SQL-запросов: фильтрация, группировка, соединение.
4. **Понимание архитектуры Spark:**
    
    - Разница между SparkContext и SparkSession.
    - RDD, DataFrame, и Dataset — как они связаны.
    - Как работает Catalyst Optimizer (логический/физический план).

### **Рекомендация по стратегии подготовки:**

1. **Сделай акцент на DataFrame API.**
    
    - Выполняй задачи и практикуй основные методы.
    - Пойми, как работает ленивое выполнение, партиционирование и шифлы.
2. **Параллельно тренируй SQL.**
    
    - Выполняй те же задачи, что и в DataFrame API, но через SQL, чтобы видеть, как они взаимодействуют.
3. **Пройди несколько тестовых собеседований.**
    
    - Это поможет понять, какой подход чаще спрашивают в реальной практике.
4. **Будь готов объяснить, почему ты выбрал определённый подход.**
    
    - Например: _"Я использую DataFrame API, потому что оно более производительное и позволяет легко интегрировать Python-логику."_