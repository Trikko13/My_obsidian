
#### **6.1 Spark UI**

1. **Что такое Spark UI?**
    
    - Это веб-интерфейс для мониторинга выполнения задач Spark.
    - Показывает:
        - DAG (Directed Acyclic Graph) — план выполнения задач.
        - Время выполнения задач.
        - Использование памяти и shuffle.
2. **Как открыть Spark UI?**
    
    - При запуске Spark локально, Spark UI доступен по адресу:
        
        `http://localhost:4040`
        
    - Для кластеров YARN UI можно посмотреть по адресу ResourceManager:
        
        `http://<ResourceManager-host>:8088`
        
3. **Разделы Spark UI:**
    
    - **Jobs:** Информация о выполнении задач, включая завершённые, активные и неудачные.
    - **Stages:** Список стадий с информацией о времени выполнения и статусе.
    - **Storage:** Кэшированные RDD и DataFrame.
    - **Environment:** Конфигурация Spark (например, `executor.memory`, `executor.cores`).
    - **SQL:** План выполнения SQL-запросов.

---

#### **6.2 Логи Spark**

1. **Где искать логи?**
    
    - Логи Spark можно найти:
        - На локальной машине — в консоли или файле логов.
        - На кластере YARN — через UI ResourceManager:

            
            `http://<ResourceManager-host>:8088`
            
2. **Настройка уровня логирования:**
    
    - Spark использует библиотеку Log4j для логов.
    - Уровень логов можно настроить в файле `log4j.properties`:
     
        
        `log4j.rootCategory=INFO, console log4j.logger.org.apache.spark=DEBUG log4j.logger.org.apache.hadoop=ERROR`
        
3. **Полезные уровни логов:**
    
    - `INFO`: Показывает базовую информацию о выполнении.
    - `DEBUG`: Даёт детальную информацию (для отладки).
    - `ERROR`: Показывает только ошибки.

---

#### **6.3 Отладка кода**

1. **Используй `explain` для SQL-запросов:**
    
    - Метод `explain` показывает план выполнения SQL-запроса.
    - Пример:
        
        
        `df = spark.read.csv("file.csv") df.select("column").explain()`
        
2. **Локальное тестирование:**
    
    - Используй небольшой набор данных для тестирования.
    - Пример запуска Spark локально:
        
        `spark-submit --master local[2] script.py`
        
3. **Проверяй данные в каждой стадии:**
    
    - Используй `.show()` или `.take()` для проверки:
        
        
        `df.show(5)  # Показывает первые 5 строк`
        

---

#### **6.4 Общие ошибки и их решение**

1. **OutOfMemoryError (GC overhead limit exceeded):**
    
    - Проблема: Недостаточно памяти для выполнения задачи.
    - Решение:
        - Увеличь память для executor:
            
            
            `spark.executor.memory = "4g"`
            
        - Уменьши количество данных на партицию:
          
            
            `rdd = rdd.coalesce(10)`
            
2. **Task not serializable:**
    
    - Проблема: Объект в RDD не может быть сериализован.
    - Решение:
        - Убедись, что все функции, переданные в RDD, сериализуемы.
3. **Shuffle spill to disk:**
    
    - Проблема: Данные shuffle не помещаются в память и записываются на диск.
    - Решение:
        - Увеличь количество партиций:
          
            
            `rdd = rdd.repartition(20)`
            

---

#### **6.5 Безопасность в Spark**

1. **Аутентификация и авторизация:**
    
    - Spark поддерживает Kerberos для защиты данных.
    - Настрой `spark.authenticate` для включения аутентификации:
        
        `spark.authenticate=true`
        
2. **Шифрование данных:**
    
    - Для защиты данных при передаче настрой:
        
        `spark.network.crypto.enabled=true`
        
3. **Доступ к Spark UI:**
    
    - Настрой пароли для защиты доступа к Spark UI.

---

### **Итоговые рекомендации**

1. Используй Spark UI для мониторинга выполнения задач и выявления узких мест.
2. Настраивай логи для детальной отладки.
3. Проводи тестирование на небольших данных перед запуском в кластер.
4. Следи за безопасностью, особенно при работе с чувствительными данными.