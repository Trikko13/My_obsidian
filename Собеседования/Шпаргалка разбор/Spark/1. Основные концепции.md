### **1. Основы Spark: концепции и компоненты (доработка)**

#### **1.1 Что такое Spark?**

Apache Spark — это распределённая вычислительная система для обработки больших данных. Она позволяет выполнять масштабируемые вычисления на кластере с высокой производительностью. Основные возможности Spark:

- Распределённая обработка больших данных.
- Высокая скорость вычислений за счёт оптимального использования памяти и минимизации операций записи на диск.
- Поддержка работы с различными форматами данных (CSV, Parquet, HDFS, S3 и др.).

**Компоненты Spark:**

- **Spark Core:** Основной движок для выполнения распределённых вычислений.
- **Spark SQL:** Модуль для работы с данными с использованием SQL.
- **Spark Streaming:** Модуль для обработки потоковых данных.
- **MLlib:** Библиотека машинного обучения.
- **GraphX:** Библиотека для анализа графов.

---

#### **1.2 Pandas vs Spark**

|**Особенность**|**Pandas**|**Spark**|
|---|---|---|
|**Тип обработки данных**|Локальная обработка|Распределённая обработка|
|**Размер данных**|Подходит для работы с небольшими наборами|Подходит для обработки больших наборов данных|
|**Область применения**|Анализ данных на локальной машине|Масштабируемая обработка в кластерах|

---

#### **1.3 SparkSession**

`SparkSession` — это точка входа для работы с данными в Spark.  
Он объединяет в себе возможности Spark SQL и DataFrame API. Пример создания:

`from pyspark.sql import SparkSession  spark = SparkSession.builder \     .appName("Example") \     .getOrCreate()`

---

#### **1.4 Основные абстракции данных в Spark**

1. **RDD (Resilient Distributed Dataset):**
    
    - Низкоуровневая абстракция для работы с данными в Spark.
    - Позволяет выполнять трансформации (`map`, `filter`) и действия (`collect`, `count`).
    - Подходит для сложных операций над распределёнными данными.
2. **DataFrame:**
    
    - Высокоуровневая структура данных, аналогичная таблице.
    - Обеспечивает API для выполнения SQL-запросов и аналитики.
3. **Dataset (в Scala и Java):**
    
    - Типизированная версия DataFrame, которая предоставляет преимущества статической типизации.

---

#### **1.5 Основные модули Spark**

1. **Spark SQL:**
    
    - Используется для выполнения SQL-запросов на распределённых данных.
    - Поддерживает форматы, такие как Parquet, JSON, и Hive.
2. **Spark Streaming:**
    
    - Подходит для обработки потоковых данных.
    - Источники: Kafka, Flume, сокеты и др.
3. **MLlib:**
    
    - Библиотека машинного обучения.
    - Поддерживает алгоритмы классификации, регрессии, кластеризации.
4. **GraphX:**
    
    - Инструмент для анализа графов и построения графовых алгоритмов.

![[спарк память в ексекьжюторе.png]]

