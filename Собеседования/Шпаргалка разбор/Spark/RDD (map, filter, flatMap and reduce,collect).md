- Как RDD обрабатывает данные распределённо?

#### 1. **Понимание RDD**

**RDD (Resilient Distributed Dataset)** — это базовая структура данных в Spark. Главные свойства:

1. **Распределённость:** RDD автоматически делится на партиции, которые обрабатываются параллельно на кластере.
2. **Устойчивость (Resilient):** Если часть данных потеряна, Spark восстанавливает её из исходных данных или выполненных операций.
#### 2. **Типы операций с RDD**

RDD поддерживает два типа операций:

- **Трансформации:** Возвращают новый RDD, но ничего не выполняют до вызова действий (ленивое вычисление).  
Примеры: `map`, `filter`, `flatMap`, `distinct`, `reduceByKey`.

Пример:
`rdd = sc.parallelize([1, 2, 3, 4])`
`squared_rdd = rdd.map(lambda x: x ** 2)  # Трансформация`
* **Действия (Actions):** Выполняют вычисления и возвращают результат.  

Примеры: `collect`, `count`, `reduce`, `take`.

Пример:
`print(squared_rdd.collect())  # Действие`

###  **Как RDD обрабатывает данные распределённо?**

#### **Процесс распределения:**

1. **Разделение данных на партиции:**
    
    - При создании RDD из коллекции (например, `parallelize`) Spark делит данные на несколько партиций.

        `rdd = sc.parallelize([1, 2, 3, 4, 5], numSlices=2)  # Делим на 2 партиции`
        
    - При загрузке данных из файла Spark разделяет данные по блокам HDFS (по умолчанию 128 МБ в Hadoop 2+).
2. **Выполнение операций на узлах:**
    
    - Каждая партиция передаётся отдельному исполнителю (executor), который обрабатывает её независимо.
    - Трансформации применяются к каждой партиции.
3. **Передача данных между узлами (shuffle):**
    
    - Если операция требует данных из разных партиций (например, `groupByKey`), Spark перемещает их между узлами (shuffle). Это дорого, и его стоит избегать.

#### **Правила при разделении работы:**

- **Избегайте дисбаланса:** Партиции должны быть равномерно распределены, иначе один узел будет перегружен.
- **Минимизируйте shuffle:** Старайтесь избегать операций, которые требуют перемещения данных между узлами (например, `join`, `groupByKey`).


###  **Коллекции в Spark**

В контексте Spark коллекция — это набор данных, с которым можно работать локально (в памяти ПК) до передачи её в Spark (в узлы кластера) для обработки. Например, это может быть список, массив или другой объект, поддерживающий итерации в Python.


---------------------------------------------------------
## **Основные методы в Spark (RDD)**

Spark предоставляет множество методов для работы с RDD. Разделим их на **трансформации** (возвращают новый RDD) и **действия** (возвращают значение или объект).

#### **Трансформации (Transformations):**

- **`map(func)`**  
    Применяет функцию `func` ко всем элементам RDD и возвращает новый RDD. 
    `rdd.map(lambda x: x * 2)
**Когда использовать?**  
    Когда нужно передать короткую функцию "на лету", не создавая её отдельно. Например, при работе с функциями `map`, `filter` или в Spark.

Пример
`rdd = sc.parallelize([1, 2, 3, 4, 5]) 
`rdd_plus_2 = rdd.map(lambda x: x + 2) 
`print(rdd_plus_2.collect()) # [3, 4, 5, 6, 7]`


- **`filter(func)`**  
    Оставляет только элементы, для которых `func` возвращает `True`.
    `rdd.filter(lambda x: x > 3)`
**Когда использовать?**  
    Для удаления ненужных данных или выделения подмножеств (например, строки с ошибками в логах).

Пример
rdd = sc.parallelize([1, 2, 3, 4, 5]) 
even_rdd = rdd.filter(lambda x: x % 2 == 0) 
print(even_rdd.collect()) # [2, 4]

- ####  **`flatMap(func)` — создаёт несколько элементов из одного**
**Описание:**  
    `flatMap` похож на `map`, но возвращает **"развёрнутый" результат**. Каждая строка или элемент может преобразоваться в несколько новых элементов.  
    Отличие от `map`:
    
    - map возвращает вложенные структуры (например, списки внутри RDD).
    - flatMap` "распаковывает" их.
**Пример с разбиением строк:** Разбиваем строки на слова:

    `rdd = sc.parallelize(["hello world", "big data"]) 
    words_rdd = rdd.flatMap(lambda x: x.split())    print(words_rdd.collect())  # ['hello', 'world', 'big', 'data']`
    
**Когда использовать?**  
    Когда нужно преобразовать данные в плоский список. Например:
    
    - Разбивать строки на слова.
    - Преобразовать запись в массив чисел.
### **Сравнение `map` и `flatMap`:**

| Функция   | Входной элемент | Выходной элемент | Развёрнутый результат |
| --------- | --------------- | ---------------- | --------------------- |
| `map`     | 1               | 1                | ❌ Нет                 |
| `flatMap` | 1               | 0, 1 или больше  | ✅ Да                  |
|           |                 |                  |                       |
#### **Пример для сравнения:**

`rdd = sc.parallelize(["hello world", "big data"])  

`#map` 
`mapped_rdd = rdd.map(lambda x: x.split())` 
`print(mapped_rdd.collect())`  `#[['hello', 'world'], ['big', 'data']]`  

`#flatMap` 
`flat_mapped_rdd = rdd.flatMap(lambda x: x.split())` 
`print(flat_mapped_rdd.collect())  # ['hello', 'world', 'big', 'data']`

### **Когда применять эти функции?**

- Используй **`map`**, если каждому элементу соответствует один новый элемент.  
    _Пример: умножение, извлечение подстроки, арифметические операции._
- Используй **`filter`**, чтобы отобрать элементы, которые соответствуют условиям.  
    _Пример: убрать строки без ошибок, оставить только положительные числа._
- Используй **`flatMap`**, если элемент должен быть преобразован в несколько новых элементов.  
    _Пример: разбиение строки на слова, преобразование JSON в ключ-значение._


- **`distinct()`**  
    Удаляет дубликаты.

    `rdd.distinct()`
    
- **`union(otherRdd)`**  
    Объединяет два RDD.
    
    `rdd.union(otherRdd)`
    
- **`reduceByKey(func)`**  
    Применяется для RDD пар (ключ, значение). Группирует элементы по ключу и применяет функцию `func`.
    
    `rdd.reduceByKey(lambda a, b: a + b)`
### **Что делает `reduceByKey`:**

1. **Группировка по ключам:** `reduceByKey` группирует все пары `(key, value)` с одинаковым ключом.
    
2. **Применение функции для объединения значений:** Для каждой группы с одинаковым ключом Spark применяет указанную функцию (в данном случае `lambda a, b: a + b`), чтобы объединить значения. Эта функция выполняется многократно, пока все значения не будут сведены к одному

### **Пример:**

#### Исходный RDD (`word_pairs_rdd`):

`[("word1", 1), ("word2", 1), ("word1", 1), ("word2", 1), ("word1", 1)]`
#### Операция:

`word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)`

#### Шаги выполнения:

1. Группировка по ключам:
    `"word1": [1, 1, 1] `
	`"word2": [1, 1]`
    
2. Применение функции `a + b` для каждого ключа:
    
    - Для `"word1"`:  
        `1 + 1 = 2` → `2 + 1 = 3`
    - Для `"word2"`:  
        `1 + 1 = 2`
3. Результат:
      `[("word1", 3), ("word2", 2)]`
    
---
### **Ключевые моменты:**

1. **"Накладывание ключа и значения":**  
    Это метафорично верно в том смысле, что все значения с одинаковым ключом сводятся к одному итоговому значению.
    
2. **Суммирование значений:**  
    Здесь `lambda a, b: a + b` суммирует значения, но ты можешь использовать любую другую функцию для объединения данных.
    
3. **Работа на уровне партиций:**
    
    - Сначала `reduceByKey` объединяет данные **в пределах каждой партиции**.
    - Затем результаты партиций объединяются между собой. Это делает операцию эффективной в распределённой среде.

### **Связь между `reduceByKey` и `groupByKey + mapValues`:**

1. **`groupByKey`:**
    
    - Группирует значения по ключам, перемещая все данные с одинаковыми ключами в одну партицию.
    - Это аналог первого этапа `reduceByKey`, когда Spark собирает данные с одинаковыми ключами.
2. **`mapValues`:**
    
    - Применяется к сгруппированным значениям, преобразуя их так, как требуется (например, превращая их в список).
    - Это эквивалент второго этапа `reduceByKey`, где Spark выполняет пользовательскую функцию над сгруппированными значениями.

### **Когда использовать `groupByKey` и `mapValues`:**

1. **Когда нельзя использовать `reduceByKey`:**
    
    - Если для обработки значений требуется их полный список (например, для составления списков, вычисления медианы или выполнения сложных операций).
    
    Пример:

    `rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 2), ("b", 2), ("a", 3)]) 
    `result = rdd.groupByKey().mapValues(lambda x: sum(x) / len(x))  # Среднее 
    `print(result.collect())`
    
2. **Для специфических задач:**
    
    - Если требуется доступ ко всем значениям в исходном виде, без предварительной агрегации.
- **`join(otherRdd)`**  
    Выполняет внутреннее соединение (join) для двух RDD пар (ключ, значение).
    
    
    `rdd1.join(rdd2)`
    
#### **Действия (Actions):**

- **`collect()`**  
    Возвращает все элементы RDD как список (аккуратно с большими данными!).
    
    `rdd.collect()`
    
- **`count()`**  
    Возвращает количество элементов в RDD.
    
    `rdd.count()`
    
- **`take(n)`**  
    Возвращает первые `n` элементов RDD.
    
    `rdd.take(3)`
    
- **`reduce(func)`**  
    Объединяет элементы RDD, используя ассоциативную функцию `func`.
- 
    `rdd.reduce(lambda a, b: a + b)`
    
- **`foreach(func)`**  
    Применяет функцию `func` ко всем элементам RDD.
    
    `rdd.foreach(print)`
