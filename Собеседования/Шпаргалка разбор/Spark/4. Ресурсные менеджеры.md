#### **4.1 Что такое YARN и как он работает в Spark?**

1. **YARN (Yet Another Resource Negotiator):**
    
    - YARN — это менеджер ресурсов, который управляет распределением ресурсов (памяти и ядер) между приложениями в кластере.
    - В Spark YARN отвечает за:
        - Запуск и контроль driver.
        - Выделение executor'ов и контроль их работы.
2. **Роли YARN в Spark:**
    
    - **ResourceManager (главный узел):**  
        Управляет ресурсами всего кластера.
    - **NodeManager (узлы-исполнители):**  
        Контролирует ресурсы на каждом сервере (executor'ы).
3. **Режимы работы Spark с YARN:**
    
    - **Cluster mode:**  
        Driver запускается на одном из узлов кластера.
    - **Client mode:**  
        Driver запускается на локальной машине (подходит для отладки и тестов).

---

#### **4.2 Основные параметры для настройки YARN**

1. **Ресурсы executor:**
    
    - Количество памяти на executor:
        
        `spark.executor.memory = "4g"`
        
    - Количество ядер на executor:
        
        `spark.executor.cores = 2`
        
2. **Количество executor'ов:**
    
    - Используй параметр `spark.executor.instances` для задания числа executor'ов:
        
        `spark.executor.instances = 10`
        
3. **Ресурсы driver:**
    
    - Количество памяти для driver:
        
        `spark.driver.memory = "1g"`
        
    - Количество ядер для driver:
        
        
        `spark.driver.cores = 1`
        

---

#### **4.3 Распределение ресурсов в кластере**

1. **Ресурсы для YARN:**
    
    - Каждый сервер выделяет ресурсы для YARN и операционной системы (по умолчанию 1 ядро и 1 ГБ памяти).
2. **Ресурсы для executor:**
    
    - Каждый executor использует часть оставшихся ресурсов:
        

        `N ядер и M ГБ памяти на сервер.`
        
3. **Ресурсы для driver:**
    
    - Driver использует 1 ядро и 1 ГБ памяти (по умолчанию).

---

#### **4.4 Пример конфигурации для кластера**

1. **Условие:**
    
    - В кластере 10 узлов.
    - У каждого узла 16 ядер и 64 ГБ памяти.
2. **Конфигурация:**
    
    `spark.executor.instances = 20         # 20 executor'ов spark.executor.cores = 4              # 4 ядра на executor spark.executor.memory = "15g"         # 15 ГБ памяти на executor spark.driver.memory = "4g"            # 4 ГБ памяти на driver spark.driver.cores = 2                # 2 ядра на driver`
    
3. **Рассчитаем использование ресурсов:**
    
    - **Executor'ы:**  
        Всего 20 executor'ов x 4 ядра = 80 ядер, 20 executor'ов x 15 ГБ = 300 ГБ памяти.
    - **Driver:**  
        2 ядра и 4 ГБ памяти.
    - Всего: 82 ядра и 304 ГБ памяти.

---

#### **4.5 Настройка YARN через Spark-submit**

Для запуска приложения с нужной конфигурацией можно использовать `spark-submit`:

`spark-submit \   --master yarn \   --deploy-mode cluster \   --executor-memory 15g \   --executor-cores 4 \   --num-executors 20 \   my_spark_app.py`

---

#### **4.6 Как выбрать оптимальные ресурсы?**

1. **Для executor:**
    
    - Используй от 2 до 5 ядер на executor (для лучшего баланса между параллелизмом и нагрузкой на планировщик).
    - Память executor должна быть кратна 4 ГБ.
2. **Для driver:**
    
    - Увеличивай память, если приложение выполняет сложные вычисления или управляет большим количеством данных.
3. **Число executor:**
    
    - Определяется доступными ресурсами кластера и требованиями приложения.

---

### **Итоговые рекомендации**

1. Используй `spark.executor.cores` и `spark.executor.memory` для настройки ресурсов executor'ов.
2. Следи за балансом между количеством executor'ов и их размерами.
3. Используй `spark-submit` для управления ресурсами в приложении.
4. Анализируй выполнение задач в Spark UI, чтобы корректировать настройки.