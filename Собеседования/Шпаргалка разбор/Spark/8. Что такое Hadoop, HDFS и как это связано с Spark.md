#### **Hadoop и его связь со Spark**

Hadoop — это экосистема для обработки больших данных. Она включает:

1. **HDFS (Hadoop Distributed File System):**  
    Распределённая файловая система для хранения больших объёмов данных.
2. **MapReduce:**  
    Модель распределённых вычислений для обработки данных в кластере.
3. **YARN (Yet Another Resource Negotiator):**  
    Менеджер ресурсов, который управляет кластерами и выделяет ресурсы для приложений, таких как Spark.

---

#### **1. Что такое HDFS?**

HDFS — основная файловая система в Hadoop. Она:

- Разделяет данные на блоки (по умолчанию **128 МБ**).
- Распределяет блоки по узлам кластера.
- Хранит несколько копий каждого блока (по умолчанию 3), чтобы защитить данные от потерь.

##### **Почему блоки 128 МБ?**

- Это баланс между накладными расходами на управление файлами и эффективностью обработки.
- Слишком мелкие блоки увеличивают нагрузку на файловую систему.
- Слишком большие блоки требуют больше ресурсов для обработки.

##### **Как Spark взаимодействует с HDFS?**

- Spark может считывать данные из HDFS через API.
- Данные распределяются между executor'ами, обеспечивая параллельную обработку.

Пример чтения файла из HDFS:

`rdd = sc.textFile("hdfs://path/to/data.txt")`

---

#### **2. Что такое MapReduce и почему Spark его заменяет?**

1. **Как работает MapReduce:**
    
    - Данные считываются из HDFS.
    - Выполняются две основные стадии:
        - **Map:** Преобразование данных.
        - **Reduce:** Агрегация данных.
    - Результаты снова записываются в HDFS.
2. **Недостатки MapReduce:**
    
    - После каждой стадии (Map и Reduce) данные записываются на диск.
    - Это вызывает высокие накладные расходы на ввод-вывод (I/O).
3. **Почему Spark быстрее:**
    
    - Spark выполняет вычисления в памяти (RDD, DataFrame), уменьшая I/O.
    - Ленивые вычисления позволяют Spark оптимизировать план выполнения задач.
    - Множество операций (например, `filter`, `reduceByKey`) выполняются быстрее, так как не требуют записи на диск.

---

#### **3. Как Spark использует YARN?**

1. **Что делает YARN:**
    
    - Выделяет ресурсы для приложений, таких как Spark.
    - Управляет задачами (запуск, мониторинг, завершение).
2. **Режимы работы Spark с YARN:**
    
    - **Cluster mode:**  
        Driver запускается на узле кластера.
    - **Client mode:**  
        Driver запускается на локальной машине.
3. **Почему YARN удобен для Spark:**
    
    - Spark может использовать YARN для управления ресурсами, такими как память и ядра.
    - Это позволяет запускать Spark-приложения в том же кластере, где хранятся данные (HDFS).

---

#### **4. Spark без Hadoop**

Spark может работать без Hadoop, используя:

- **Локальные файлы:**  
    Spark может обрабатывать данные с локального диска:
    
    `rdd = sc.textFile("file:///path/to/local/data.txt")`
    
- **Базы данных:**  
    Spark поддерживает подключение через JDBC.
- **Облачные хранилища:**  
    Данные могут быть загружены из S3, GCS и других источников.

Однако Hadoop остаётся популярным, так как HDFS — это надёжная файловая система для больших данных, а YARN упрощает управление ресурсами.

---

### **Итог**

1. **Hadoop и Spark дополняют друг друга:**
    
    - HDFS используется для хранения данных.
    - YARN управляет ресурсами в кластере.
2. **Почему Spark лучше MapReduce:**
    
    - Spark работает быстрее за счёт использования памяти и ленивых вычислений.
3. **Spark может работать и без Hadoop:**
    
    - Он универсален и поддерживает локальные файлы, базы данных и облачные хранилища.