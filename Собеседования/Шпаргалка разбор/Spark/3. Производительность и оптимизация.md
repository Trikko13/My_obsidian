#### **3.1 Ленивые вычисления**

1. **Что такое ленивые вычисления?**
    
    - Spark **ленивый**, то есть он не выполняет трансформации, пока не встретит действие (action).
    - Трансформации, такие как `map` или `filter`, создают новый RDD, но не выполняются сразу. Вместо этого Spark строит граф задач (DAG).
    - Действия, такие как `collect` или `count`, заставляют Spark выполнить DAG.
2. **Преимущество:**
    
    - Spark может оптимизировать выполнение, объединяя трансформации и минимизируя операции shuffle.
3. **Пример:**
    
    `rdd = sc.parallelize(range(1, 11)) transformed_rdd = rdd.map(lambda x: x * 2).filter(lambda x: x > 10)  # Ничего не произойдёт, пока не вызвано действие print(transformed_rdd.collect())  # Действие запускает выполнение`
    

---

#### **3.2 Shuffle и его минимизация**

1. **Что такое shuffle?**
    
    - Shuffle — это перемешивание данных между executor'ами, чтобы распределить их по партициям.
    - Операции shuffle:
        - `join`
        - `groupByKey`
        - `reduceByKey`
        - `orderBy`
2. **Почему shuffle дорогой:**
    
    - Требует записи данных на диск и передачи их по сети.
    - Увеличивает накладные расходы, особенно при большом количестве данных.
3. **Как минимизировать shuffle:**
    
    - Используй `reduceByKey` вместо `groupByKey`:
        
        `rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3)]) # Плохой вариант: groupByKey вызывает shuffle grouped = rdd.groupByKey() # Лучше: reduceByKey минимизирует shuffle reduced = rdd.reduceByKey(lambda a, b: a + b)`
        
    - Сокращай количество партиций после shuffle:
        
        `repartitioned_rdd = rdd.repartition(10)`
        

---

#### **3.3 Оптимальное количество партиций**

1. **Общее правило:**
    
    - Используй 2–3 партиции на каждое ядро.
    - Формула:
        
        
        `numPartitions ≈ total_cores * 2–3`
        
2. **Как проверить текущее количество партиций:**
    
    
    `print(rdd.getNumPartitions())`
    
3. **Как изменить количество партиций:**
    
    - Увеличение числа партиций:
        
        `rdd = rdd.repartition(8)`
        
    - Уменьшение числа партиций:
        

        
        `rdd = rdd.coalesce(4)`
        

---

#### **3.4 Кэширование и `persist`**

1. **Почему это важно?**
    
    - Если RDD или DataFrame используется несколько раз, кэширование ускоряет выполнение, сохраняя данные в памяти.
2. **Кэширование:**
    
    - Используй `cache`, чтобы сохранить данные в памяти:
        
        `cached_rdd = rdd.cache() print(cached_rdd.count())  # Данные кэшируются после первого действия`
        
3. **`persist`:**
    
    - Расширяет возможности кэширования, позволяя выбрать уровень хранения:
        
        
        `persisted_rdd = rdd.persist(StorageLevel.MEMORY_AND_DISK)`
        
4. **Spark UI:**
    
    - Кэшированные данные отображаются в Spark UI, где можно проверить использование памяти.

---

#### **3.5 Шардинг (распределение данных по ключам)**

1. **Что это такое?**
    
    - Шардинг — это распределение данных по ключам между партициями.
    - Помогает минимизировать перекос данных (`data skew`), когда некоторые ключи содержат слишком много записей.
2. **Как выполнить шардинг:**
    
    - Используй `partitionBy` для равномерного распределения данных:

        `rdd = rdd.partitionBy(4, lambda key: hash(key) % 4)`
        

---

#### **3.6 Мониторинг производительности**

1. **Spark UI:**
    
    - Отслеживай задачи, стадии, использование памяти и shuffle.
    - Адрес по умолчанию: `http://localhost:4040`.
2. **Анализ логов:**
    
    - Логи Spark помогут найти узкие места и ошибки.
    - Настройка уровня логирования:
        
  
        
        `log4j.rootCategory=INFO, console`
        

---

### **Итоговые рекомендации**

1. Минимизируй shuffle с помощью `reduceByKey` и правильного распределения данных.
2. Настраивай партиции для оптимального параллелизма.
3. Используй кэширование и `persist` для повторно используемых данных.
4. Анализируй производительность с помощью Spark UI и логов.