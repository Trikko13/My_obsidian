#### **5.1 Чтение данных в Spark**

Spark поддерживает множество форматов данных, таких как CSV, JSON, Parquet, ORC, Avro, HDFS и S3. Основной интерфейс для чтения данных — это `SparkSession`.

##### **Чтение данных из файлов**

1. **CSV:**
    
    `df = spark.read.format("csv").option("header", "true").load("path/to/file.csv") df.show()`
    
2. **Parquet:**
    `df = spark.read.format("parquet").load("path/to/file.parquet")`
    
3. **JSON:**
    
    `df = spark.read.format("json").load("path/to/file.json")`
    
4. **HDFS:**
    
    `rdd = sc.textFile("hdfs://path/to/file.txt")`
    

##### **Чтение данных из базы данных**

Spark также поддерживает подключение к базам данных через JDBC:

`df = spark.read.format("jdbc") \     .option("url", "jdbc:postgresql://host:port/dbname") \     .option("dbtable", "table_name") \     .option("user", "username") \     .option("password", "password") \     .load() df.show()`

---

#### **5.2 Запись данных**

##### **Форматы записи**

1. **CSV:**
    
    `df.write.format("csv").option("header", "true").save("path/to/output")`
    
2. **Parquet:**
    
    `df.write.format("parquet").save("path/to/output")`
    
3. **JSON:**
    
    `df.write.format("json").save("path/to/output")`
    
4. **HDFS:**
    
    `rdd.saveAsTextFile("hdfs://path/to/output")`
    

##### **Режимы записи**

- **append:** Добавить данные в существующий файл.
- **overwrite:** Перезаписать файл.
- **error (default):** Выбросить ошибку, если файл существует.
- **ignore:** Игнорировать запись, если файл существует.

**Пример:**


`df.write.format("csv") \     .mode("overwrite") \     .option("header", "true") \     .save("path/to/output")`

---

#### **5.3 Как Spark распределяет данные при записи?**

1. **Количество файлов:**
    
    - Spark создаёт отдельный файл для каждой партиции.
    - Если у RDD или DataFrame 100 партиций, Spark создаст 100 файлов.
2. **Проблемы большого количества файлов:**
    
    - Множество мелких файлов увеличивает нагрузку на файловую систему.
    - Сложнее управлять и обрабатывать такие данные.
3. **Решение:**
    
    - Уменьшай количество партиций перед записью:
        
        `df = df.repartition(10) df.write.format("csv").save("path/to/output")`
        

---

#### **5.4 Пример записи данных**

##### **Сценарий:**

- У тебя есть DataFrame с данными о продажах. Нужно записать их в Parquet, разделяя данные по годам.

##### **Код:**

`from pyspark.sql import SparkSession`  

`spark = SparkSession.builder.appName("WriteExample").getOrCreate()`  

`Создаём пример DataFrame` 
`data = [("2022", "Product A", 100), ("2023", "Product B", 200)]` 
`columns = ["Year", "Product", "Revenue"]`  

`df = spark.createDataFrame(data, columns)` 

`Записываем данные в Parquet, разделяя по столбцу Year df.write.partitionBy("Year").format("parquet").save("path/to/output")`

---

#### **5.5 Особенности чтения и записи в распределённых системах**

1. **HDFS:**
    
    - Spark использует HDFS для хранения больших данных. Размер блока по умолчанию — 128 МБ.
    - Чтение больших файлов из HDFS автоматически создаёт оптимальное количество партиций.
2. **S3:**
    
    - Для работы с S3 в Spark нужны AWS-креденшлы.
    - Пример чтения:
        
        `df = spark.read.csv("s3a://bucket-name/path/to/file.csv")`
        
3. **Parquet и ORC:**
    
    - Эти форматы эффективны для аналитики, так как поддерживают схему и сжатие.
    - Рекомендуется использовать их для хранения больших данных.

---

### **Итоговые рекомендации**

1. Выбирай формат записи исходя из требований (CSV для текстовых данных, Parquet для аналитики).
2. Уменьшай количество партиций перед записью, чтобы избежать множества мелких файлов.
3. Используй `partitionBy` для улучшения производительности чтения при анализе.