#### **5.1 Чтение данных в Spark**

Spark поддерживает множество форматов данных, таких как CSV, JSON, Parquet, ORC, Avro, HDFS и S3. Основной интерфейс для чтения данных — это `SparkSession`.

##### **Чтение данных из файлов**

1. **CSV:**
    
    `df = spark.read.format("csv").option("header", "true").load("path/to/file.csv") df.show()`
    
2. **Parquet:**
    `df = spark.read.format("parquet").load("path/to/file.parquet")`
    
3. **JSON:**
    
    `df = spark.read.format("json").load("path/to/file.json")`
    
4. **HDFS:**
    
    `rdd = sc.textFile("hdfs://path/to/file.txt")`
    

##### **Чтение данных из базы данных**

Spark также поддерживает подключение к базам данных через JDBC:

`df = spark.read.format("jdbc") \     .option("url", "jdbc:postgresql://host:port/dbname") \     .option("dbtable", "table_name") \     .option("user", "username") \     .option("password", "password") \     .load() df.show()`

---

#### **5.2 Запись данных**

##### **Форматы записи**

1. **CSV:**
    
    `df.write.format("csv").option("header", "true").save("path/to/output")`
    
2. **Parquet:**
    
    `df.write.format("parquet").save("path/to/output")`
    
3. **JSON:**
    
    `df.write.format("json").save("path/to/output")`
    
4. **HDFS:**
    
    `rdd.saveAsTextFile("hdfs://path/to/output")`
    

##### **Режимы записи**

- **append:** Добавить данные в существующий файл.
- **overwrite:** Перезаписать файл.
- **error (default):** Выбросить ошибку, если файл существует.
- **ignore:** Игнорировать запись, если файл существует.

**Пример:**


`df.write.format("csv") \     .mode("overwrite") \     .option("header", "true") \     .save("path/to/output")`

---

#### **5.3 Как Spark распределяет данные при записи?**

1. **Количество файлов:**
    
    - Spark создаёт отдельный файл для каждой партиции.
    - Если у RDD или DataFrame 100 партиций, Spark создаст 100 файлов.
2. **Почему большое количество партиций замедляет запись?**
    
    - Множество мелких файлов увеличивает нагрузку на файловую систему.
    - Сложнее управлять такими данными, их загрузка в будущем будет замедлена.
3. **Как оптимизировать:**
    
    - Уменьшай количество партиций с помощью `coalesce`, если данных немного:
        
        `rdd = sc.parallelize(range(1, 10001), numSlices=100) rdd.coalesce(10).saveAsTextFile("output")`
        
    - Увеличивай количество партиций с помощью `repartition`, если данные распределены неравномерно:
        
        `balanced_rdd = rdd.repartition(10) balanced_rdd.saveAsTextFile("output")`
        
4. **Когда уменьшать партиции:**
    
    - Данные небольшие, и нужно создать меньше файлов.
    - Данные уже отсортированы или агрегированы.
5. **Когда увеличивать партиции:**
    
    - Если данные распределены неравномерно.
    - Одна или несколько партиций перегружены.

---

#### **5.4 Пример записи данных**

##### **Сценарий:**

- У тебя есть DataFrame с данными о продажах. Нужно записать их в Parquet, разделяя данные по годам.

##### **Код:**

`from pyspark.sql import SparkSession`  

`spark = SparkSession.builder.appName("WriteExample").getOrCreate()`  

`Создаём пример DataFrame` 
`data = [("2022", "Product A", 100), ("2023", "Product B", 200)]` 
`columns = ["Year", "Product", "Revenue"]`  

`df = spark.createDataFrame(data, columns)` 

`Записываем данные в Parquet, разделяя по столбцу Year df.write.partitionBy("Year").format("parquet").save("path/to/output")`

---

#### **5.5 Особенности чтения и записи в распределённых системах**

1. **HDFS:**
    
    - Spark использует HDFS для хранения больших данных. Размер блока по умолчанию — 128 МБ.
    - Чтение больших файлов из HDFS автоматически создаёт оптимальное количество партиций.
2. **S3:**
    
    - Для работы с S3 в Spark нужны AWS-креденшлы.
    - Пример чтения:
        
        `df = spark.read.csv("s3a://bucket-name/path/to/file.csv")`
        
3. **Parquet и ORC:**
    
    - Эти форматы эффективны для аналитики, так как поддерживают схему и сжатие.
    - Рекомендуется использовать их для хранения больших данных.

---

### **Итоговые рекомендации**

- Выбирай формат записи исходя из требований:
    
    - **CSV:** Подходит для текстовых данных и совместимости с другими инструментами.
    - **Parquet:** Эффективен для аналитики и больших данных.
    - **JSON:** Удобен для интеграции с API.
- Уменьшай количество партиций перед записью, чтобы избежать множества мелких файлов:
    
    `df.coalesce(10).write.format("csv").save("path/to/output")`
    
- Используй `partitionBy`, чтобы разделить данные по ключам и ускорить будущую аналитическую обработку:
    
    `df.write.partitionBy("Year").format("parquet").save("path/to/output")`
    
- Настраивай количество партиций в зависимости от данных:
    
    - Для небольших данных: уменьши число партиций с помощью `coalesce`.
    - Для неравномерно распределённых данных: увеличь число партиций с помощью `repartition`.
- Поддерживай оптимальный размер партиций для HDFS:
    
    - Рекомендуется около 128 МБ на партицию.