### 1. **[[Неделя 1 (RDD)]]: Разбираемся с базой (RDD)**

- **Что изучить:**
    
    - Что такое RDD и как Spark работает с данными.
    - Основные операции с RDD: трансформации (например, `map`, `filter`, `flatMap`) и действия (`reduce`, `collect`).
- **Что сделать:**
    
    - В Jupyter Notebook напиши пару скриптов для создания RDD и выполнения операций:
        
        
        `from pyspark import SparkContext 
    - `sc = SparkContext("local", "First RDD Example") 
    -  
    - `# Пример RDD 
    - `data = [1, 2, 3, 4, 5] 
    - `rdd = sc.parallelize(data)  
    - 
    - `# Пример трансформаций и действий 
    - `squared_rdd = rdd.map(lambda x: x**2) 
    - `print(squared_rdd.collect())`
        
    - Потренируйся с небольшими датасетами (например, списки чисел или текстовые файлы).
- **Полезный вопрос к себе:**
    
    - Как RDD обрабатывает данные распределённо?
- **Время на неделю:**  
    1–2 часа в день, не больше. Если чувствуешь, что перегружаешься, просто запускай готовые примеры.
    

---

### 2. **Неделя 2: DataFrame и SQL**

- **Что изучить:**
    
    - Разница между RDD и DataFrame. Почему DataFrame проще и мощнее для аналитики.
    - Основные операции с DataFrame (`select`, `filter`, `groupBy`, `agg`).
    - Использование SQL внутри Spark.
- **Что сделать:**
    
    - Возьми CSV-файл (например, данные о продажах, погоды или что-то простое) и загрузи его в DataFrame:
        
        python
        
        Копировать код
        
        `from pyspark.sql import SparkSession  spark = SparkSession.builder.appName("DataFrame Example").getOrCreate() df = spark.read.csv("path/to/file.csv", header=True, inferSchema=True)  df.show()  # Посмотреть первые строки df.select("column_name").show()  # Выбрать колонку df.filter(df["column_name"] > 10).show()  # Фильтр`
        
    - Выполни пару SQL-запросов:
        
        python
        
        Копировать код
        
        `df.createOrReplaceTempView("table_name") result = spark.sql("SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name") result.show()`
        
- **Полезный вопрос к себе:**
    
    - Какие преимущества DataFrame перед RDD?
- **Время на неделю:**  
    Опять же, 1–2 часа в день. На выходных можно устроить "демо-день", чтобы просто повторить то, что делал.
    

---

### 3. **Неделя 3: Партиционирование и производительность**

- **Что изучить:**
    
    - Как данные делятся на партиции в Spark.
    - Что такое `repartition` и `coalesce`.
    - Как неправильное количество партиций может ухудшить производительность.
- **Что сделать:**
    
    - Попробуй изменить количество партиций в DataFrame или RDD и посмотри на результат:
        
        python
        
        Копировать код
        
        `df = df.repartition(4)  # Увеличиваем количество партиций rdd = rdd.coalesce(1)  # Уменьшаем количество партиций`
        
    - Проверь, сколько партиций используется:
        
        python
        
        Копировать код
        
        `print(rdd.getNumPartitions())`
        
- **Полезный вопрос к себе:**
    
    - Когда лучше использовать больше партиций?
- **Время на неделю:**  
    Чуть больше внимания теории (около 30 минут в день). Практика — на выходных.
    

---

### 4. **Общие рекомендации**

1. **Не пытайся сразу охватить всё:**  
    Работа с RDD, DataFrame и SQL уже охватывает большой блок знаний. Если чувствуешь, что перегружаешься, просто остановись на текущем уровне и повторяй материал.
    
2. **Используй маленькие датасеты:**  
    Для начала достаточно работать с простыми файлами, например, 10–100 строк.
    
3. **Чередуй теорию и практику:**  
    Если теория перегружает, попробуй больше времени уделять практике, и наоборот.
    
4. **Отдыхай:**  
    Работай не более 2 часов в день. Лучше сделать меньше, но качественнее.
    
5. **Пробуй разбирать ошибки:**  
    Spark будет кидать ошибки — это нормально. Находи решение через логи и вопросы в интернете.