### **Задача 1: Работа с числами**

**Цель:** Создать RDD, выполнить несколько трансформаций и действий.

#### **Шаги:**

1. Создайте RDD из списка чисел `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`.
2. Уберите нечётные числа.
3. Умножьте оставшиеся числа на 10.
4. Соберите результат в список и выведите его.

Решение:

`from pyspark.sql import SparkSession`

`#создадим SparkSession локально с именем и присвоим значение переменной spark`

`spark=SparkSession.builder.master('local').appName('My_first_RDD').getOrCreate()`

`#обращаемся SparkContext т.к. он уже внутри SparkSession и без него RDD не сделать`

`sc = spark.sparkContext`

`Список для создания RDD`

`dataset = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`
`rdd = sc.parallelize(dataset)`

`Применим трансорфмацию`
`even_numbers_rdd = rdd.filter(lambda x: x % 2 == 0)`

`Собираем результат`
`print(even_numbers_rdd.collect())`

### **Почему использовать `filter`, а не `map`?**

- **`map`**: Преобразует каждый элемент RDD, возвращая для него одно значение или структуру. Если вы используете `map` с `dataset`, это создаёт списки для каждого элемента.
- **`filter`**: Отбирает только те элементы RDD, которые соответствуют условию (в данном случае, чётные числа).

### **Сравнение `map` и `filter`:**

|**Функция**|**Что делает**|**Когда использовать**|
|---|---|---|
|**`map`**|Преобразует каждый элемент RDD|Когда нужно изменить все элементы|
|**`filter`**|Убирает элементы, которые не соответствуют условию|Когда нужно оставить только определённые элементы|
### **Задача 2: Работа с текстом**

**Цель:** Обработать строки текста, найти слова и подсчитать их длину.

#### **Шаги:**

1. Создайте RDD из списка строк:
`["Spark is great", "I love big data", "RDD is powerful"]
2. Разбейте строки на отдельные слова.
3. Вычислите длину каждого слова
4. Соберите и выведите результат.
   
`from pyspark.sql import SparkSession`

`#Создаём SparkSession`
`spark=SparkSession.builder.master('local').appName('Task_2').getOrCreate()`

`#Получаем SparkContext`
`sc = spark.sparkContext`

`#Создаём RDD из строк`
`data = ["Spark is great", "I love big data", "RDD is powerful"]`
`rdd = sc.parallelize(data)`

`#Разбиваем строки на слова`
`words_rdd = rdd.flatMap(lambda line: line.split())`

`#Вычисялемд лину кжадого слова`
`word_length_rdd = words_rdd.map(lambda word: (word,len(word)))`

`print(word_length_rdd.collect())`

`#Останавливаем SparkSession`
`spark.stop()`

### **Задача 3: Подсчёт уникальных элементов**

**Цель:** Определить количество уникальных чисел в списке.

#### **Шаги:**

1. Создайте RDD из списка чисел с дубликатами:
    [1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 7]
- Удалите дубликаты.
- Подсчитайте количество уникальных чисел.


`from pyspark.sql import SparkSession`

`#Создаём SparkSession`
`spark=SparkSession.builder.master('local').appName('Task_3').getOrCreate()`

`#Получаем SparkContext`
`sc = spark.sparkContext`

`#Создаём RDD из строк`
`data = [1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 7]`
`rdd = sc.parallelize(data)`

`#Удаляем дубликаты`
`unique_rdd = rdd.distinct()`

`#Подсчитываем количество уникальных чисел`
`unique_count =  unique_rdd.count()`
`print(f'Количество уникальных чисел: {unique_count}')` 

`#Останавливаем SparkSession`
`spark.stop()`


### **Задача 4: поработать с оконными функцияями. Добавь новый столбец с ранжированием сотрудников по возрасту внутри департамента (используй `row_number`).

`from pyspark.sql.window import Window`
`from pyspark.sql.functions import row_number`
`from pyspark.sql.functions import when`
`from pyspark.sql import SparkSession`


`# Создаём SparkSession`
`spark = SparkSession.builder.master("local").appName("DataFrame Practice").getOrCreate()`

`# Данные`
`data = [("Alice", 34, "HR"),` 
        `("Bob", 23, "IT"),` 
        `("Cathy", 45, "Finance"),` 
        `("David", 29, "IT")]`
`columns = ['name', 'age', 'department']`

`# Создаение DataFrame`
`df = spark.createDataFrame(data,schema=columns)`
`df.show()`

`# Регитстрируем DataFrame как временную таблицу`
`df.createOrReplaceTempView('people')`

`# Добавляем таблец с ранжированием через оконные функции`
`window_spec = Window.partitionBy('department').orderBy(df['age'].desc())`
`df = df.withColumn('rank', row_number().over(window_spec))`
`df.show()`


