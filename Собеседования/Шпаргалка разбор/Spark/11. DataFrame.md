DataFrame — это высокоуровневая структура данных в Spark, аналогичная таблицам в реляционных базах данных или DataFrame в Pandas.

- **Структура:**  
    Содержит строки и столбцы, где каждый столбец имеет имя и тип данных.
- **Преимущества:**
    - Удобен для обработки структурированных данных.
    - Оптимизирован с помощью Catalyst Optimizer.
    - Поддерживает интеграцию с SQL.

---

#### **2. Основные операции с DataFrame**

|**Операция**|**Описание**|**Пример**|
|---|---|---|
|**`select`**|Выборка столбцов.|`df.select("col1", "col2").show()`|
|**`filter`**|Фильтрация строк.|`df.filter(df["age"] > 30).show()`|
|**`groupBy`**|Группировка данных.|`df.groupBy("department").count().show()`|
|**`withColumn`**|Создание нового столбца.|`df.withColumn("new_col", df["age"] + 5).show()`|
|**`orderBy`**|Сортировка строк.|`df.orderBy("age", ascending=False).show()`|
|**`join`**|Объединение двух DataFrame.|`df1.join(df2, df1["id"] == df2["id"], "inner").show()`|

---

#### **3. SQL в Spark**

Spark позволяет писать SQL-запросы прямо в коде.

**Пример:**

1. Зарегистрируем DataFrame как временную таблицу:
    
    `df.createOrReplaceTempView("people")`
    
2. Выполним SQL-запрос:
    
    `result = spark.sql("SELECT * FROM people WHERE age > 30") result.show()`

### Синтаксис из заданий в GPT

`**from pyspark.sql import SparkSession`**
**`from pyspark.sql.functions import when**`


`# Создаём SparkSession`
`spark = SparkSession.builder.master("local").appName("DataFrame Practice").getOrCreate()`

`# Данные`
`data = [("Alice", 34, "HR"),` 
        `("Bob", 23, "IT"),` 
        `("Cathy", 45, "Finance"),` 
        `("David", 29, "IT")]`
`columns = ['name', 'age', 'department']`

`# Создаение DataFrame`
`df = spark.createDataFrame(data,schema=columns)`
`df.show()`

`# Регитстрируем DataFrame как временную таблицу`
`df.createOrReplaceTempView('people')`

`# Выполним SQL- запрос для поиска количествал юдей в каждом депараменте`
`result = spark.sql("SELECT count(name) AS eployee_count,department FROM people GROUP BY department")`
`result.show()`

`# Добавляем новый столбец 'seniority' с условием`
`df = df.withColumn(`
    `'seniority',`
    `when(df['age'] > 40, 'Senior').otherwise('Junior')`
`)`

`df.show()`

`# Выполним SQL- запрос для поиска сотрудников IT`
`result_1 = spark.sql("SELECT name FROM people WHERE department='IT'")`
`result_1.show()`